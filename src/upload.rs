use axum::{extract::{State, ConnectInfo, Query, Extension}, response::IntoResponse, http::{HeaderMap, StatusCode}, Json};
use axum_typed_multipart::{TryFromMultipart, TypedMultipart, FieldData};
use sqlx::{SqlitePool, Row};
use std::{fs, path::PathBuf, net::SocketAddr, collections::HashMap};
use tokio::io::AsyncWriteExt;
use serde::Deserialize;
use crate::{db, AppConfig};

#[derive(Deserialize)]
pub struct HeartbeatRequest {
    pub upload_ids: Vec<i64>,
}

#[derive(TryFromMultipart)]
pub struct ChunkUploadRequest {
    pub filename: String,
    pub chunk_index: u32,
    pub total_chunks: u32,
    pub chunk: FieldData<bytes::Bytes>,
}

pub async fn handle_chunk_upload(
    State(pool): State<SqlitePool>,
    ConnectInfo(addr): ConnectInfo<SocketAddr>,
    Extension(config): Extension<AppConfig>,
    TypedMultipart(upload_data): TypedMultipart<ChunkUploadRequest>,
) -> Result<impl IntoResponse, (StatusCode, String)> {
    // ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì—°ê²° ëŠì–´ì§ ê°ì§€ë¥¼ ìœ„í•œ future ìƒì„±
    let upload_future = process_chunk_upload(pool.clone(), addr, config, upload_data);
    
    // ì—°ê²° ëŠì–´ì§ì´ë‚˜ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬
    match tokio::time::timeout(std::time::Duration::from_secs(300), upload_future).await {
        Ok(result) => result,
        Err(_) => {
            // íƒ€ì„ì•„ì›ƒ ë°œìƒ (í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠì–´ì§ ê°€ëŠ¥ì„±)
            println!("âš ï¸ Upload timeout - client may have disconnected");
            Err((StatusCode::REQUEST_TIMEOUT, "Upload timeout".to_string()))
        }
    }
}

async fn process_chunk_upload(
    pool: SqlitePool,
    addr: SocketAddr,
    config: AppConfig,
    upload_data: ChunkUploadRequest,
) -> Result<impl IntoResponse, (StatusCode, String)> {
    let save_dir = &config.upload_dir;
    fs::create_dir_all(save_dir)
        .map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, format!("Failed to create directory: {}", e)))?;

    let client_ip = addr.ip().to_string();
    
    // ê¸°ì¡´ ì—…ë¡œë“œ ì •ë³´ í™•ì¸ (db::init_upload í˜¸ì¶œ ì „ì—)
    let existing_upload = sqlx::query("SELECT id, size FROM uploads WHERE filename = ?1 AND client_ip = ?2 AND status != 'complete'")
        .bind(&upload_data.filename)
        .bind(&client_ip)
        .fetch_optional(&pool).await
        .expect("select failed");
    
    let id = db::init_upload(&pool, &upload_data.filename, &client_ip).await;
    
    // ì´ íŒŒì¼ í¬ê¸° ê³„ì‚° ë° ì²´í¬
    let estimated_file_size = (upload_data.chunk.contents.len() as u64) * (upload_data.total_chunks as u64);
    if estimated_file_size > config.max_file_size {
        return Err((StatusCode::PAYLOAD_TOO_LARGE, format!("File too large: {} bytes exceeds limit of {} bytes", estimated_file_size, config.max_file_size)));
    }
    
    let tmp_path = PathBuf::from(save_dir).join(format!("{}.part", upload_data.filename));
    let mut file = tokio::fs::OpenOptions::new()
        .create(true)
        .append(true)
        .open(&tmp_path)
        .await
        .map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, format!("Failed to open file: {}", e)))?;

    // ì—…ë¡œë“œ ì„¸ì…˜ì˜ ì²« ë²ˆì§¸ ì²­í¬ì¸ì§€ í™•ì¸ (staticìœ¼ë¡œ ì¶”ì )
    use std::sync::{Mutex, LazyLock};
    use std::collections::HashSet;
    static LOGGED_UPLOADS: LazyLock<Mutex<HashSet<i64>>> = LazyLock::new(|| Mutex::new(HashSet::new()));
    
    {
        let mut logged = LOGGED_UPLOADS.lock().unwrap();
        if !logged.contains(&id) {
            logged.insert(id);
            
            if let Some(row) = existing_upload {
                let existing_size: i64 = row.get("size");
                if existing_size > 0 {
                    println!("ğŸ”„ Resuming upload: {} (from {} bytes, chunk {})", upload_data.filename, existing_size, upload_data.chunk_index);
                } else {
                    println!("â–¶ï¸ Starting upload: {}", upload_data.filename);
                }
            } else {
                println!("â–¶ï¸ Starting upload: {}", upload_data.filename);
            }
        }
    }

    let chunk_data = &upload_data.chunk.contents;
    if !chunk_data.is_empty() {
        file.write_all(chunk_data)
            .await
            .map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, format!("Failed to write chunk: {}", e)))?;
        db::mark_uploading(&pool, id, chunk_data.len() as i64).await;
    }

    if upload_data.chunk_index + 1 == upload_data.total_chunks {
        let final_path = PathBuf::from(save_dir).join(&upload_data.filename);
        tokio::fs::rename(&tmp_path, &final_path)
            .await
            .map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, format!("Failed to finalize file: {}", e)))?;
        println!("âœ… Completed upload: {:?}", final_path);
        db::mark_complete(&pool, id).await;
    }

    Ok(id.to_string())
}

pub async fn handle_upload_head(
    State(pool): State<SqlitePool>,
    ConnectInfo(addr): ConnectInfo<SocketAddr>,
    Query(params): Query<HashMap<String, String>>,
) -> impl IntoResponse {
    let filename = params.get("filename").unwrap_or(&"".to_string()).clone();
    let client_ip = addr.ip().to_string();
    
    // ê°™ì€ IPì—ì„œ ì§„í–‰ ì¤‘ì¸ ì—…ë¡œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
    if let Ok(Some(row)) = sqlx::query("SELECT size FROM uploads WHERE filename = ?1 AND client_ip = ?2 AND status != 'complete'")
        .bind(&filename)
        .bind(&client_ip)
        .fetch_optional(&pool).await {
        
        let uploaded_bytes: i64 = row.try_get("size").unwrap_or(0);
        let mut headers = HeaderMap::new();
        headers.insert("x-uploaded-bytes", uploaded_bytes.to_string().parse().unwrap());
        return (headers, "").into_response();
    }
    
    // ì§„í–‰ ì¤‘ì¸ ì—…ë¡œë“œê°€ ì—†ìœ¼ë©´ 0ë°”ì´íŠ¸
    let mut headers = HeaderMap::new();
    headers.insert("x-uploaded-bytes", "0".parse().unwrap());
    (headers, "").into_response()
}

pub async fn handle_heartbeat(
    State(pool): State<SqlitePool>,
    ConnectInfo(addr): ConnectInfo<SocketAddr>,
    headers: HeaderMap,
    Json(request): Json<HeartbeatRequest>,
) -> Result<impl IntoResponse, (StatusCode, String)> {
    let client_ip = addr.ip().to_string();
    let now = chrono::Utc::now().to_rfc3339();
    
    // User-Agent í—¤ë” ì¶”ì¶œ
    let user_agent = headers.get("user-agent")
        .and_then(|v| v.to_str().ok());
    
    // í´ë¼ì´ì–¸íŠ¸ heartbeat ì—…ë°ì´íŠ¸
    db::update_client_heartbeat(&pool, &client_ip, user_agent).await;
    
    let mut updated_count = 0;
    
    for upload_id in request.upload_ids {
        // ê° IDì˜ ì—…ë¡œë“œê°€ ê°™ì€ IPì—ì„œ ì§„í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ê³  updated_at ê°±ì‹ 
        match sqlx::query(
            r#"UPDATE uploads 
               SET updated_at = ?1 
               WHERE id = ?2 AND client_ip = ?3 AND status = 'uploading'"#)
            .bind(&now)
            .bind(upload_id)
            .bind(&client_ip)
            .execute(&pool).await {
            
            Ok(result) => {
                updated_count += result.rows_affected();
            },
            Err(e) => {
                println!("Heartbeat error for upload {}: {}", upload_id, e);
            }
        }
    }
    
    Ok(format!("heartbeat_ok:{}", updated_count))
}
